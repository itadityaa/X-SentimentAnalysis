{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Kaggle API key\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = 'C:/Users/itadi/Desktop/Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Kaggle\n",
    "def downloadDataset():\n",
    "\n",
    "    \"\"\"\n",
    "    Download dataset from Kaggle\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = 'kazanova/sentiment140'\n",
    "\n",
    "    downloadPath = './dataset'\n",
    "    if not os.path.exists(downloadPath):\n",
    "        os.makedirs(downloadPath)\n",
    "\n",
    "    for file in tqdm(os.listdir(downloadPath), desc=\"Processing files\"):\n",
    "        filePath = os.path.join(downloadPath, file)\n",
    "        try:\n",
    "            if os.path.isfile(filePath):\n",
    "                os.unlink(filePath)\n",
    "                print('Old files deleted successfully!')\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {filePath}. Reason: {e}')\n",
    "        \n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.dataset_download_files(dataset, path=downloadPath, unzip=True)\n",
    "\n",
    "    print('Dataset downloaded successfully!')\n",
    "\n",
    "# Download dataset\n",
    "downloadDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords');\n",
    "# Print stopwords in English\n",
    "# Does not add any value to the text data\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "dataset = pd.read_csv('./dataset/training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataset\n",
    "print(f'Shape of the dataset: {dataset.shape}')\n",
    "# Display columns in the dataset\n",
    "print(f'Columns in the dataset: {dataset.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows of the dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(f'Missing values in the dataset: \\n{dataset.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values in the target column\n",
    "print(f'Unique values in the target column: {dataset.target.unique()}')\n",
    "# Print value counts in the target column\n",
    "print(f'Value counts in the target column: \\n{dataset.target.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting target values to 0 and 1\n",
    "dataset['target'] = dataset['target'].replace({4: 1})\n",
    "\n",
    "# 0: Negative sentiment\n",
    "# 1: Positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the date is in datetime format\n",
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "\n",
    "# Extract date-related features\n",
    "dataset['year'] = dataset['date'].dt.year\n",
    "dataset['month'] = dataset['date'].dt.month\n",
    "dataset['day'] = dataset['date'].dt.day\n",
    "dataset['hour'] = dataset['date'].dt.hour\n",
    "dataset['day_of_week'] = dataset['date'].dt.day_name()\n",
    "\n",
    "# Plot tweet frequency by day\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='day_of_week', data=dataset, order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "plt.title('Tweets by Day of the Week')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URLs, special characters, and mentions\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)     # Remove hashtags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    return text\n",
    "\n",
    "dataset['clean_text'] = dataset['text'].apply(clean_text)\n",
    "\n",
    "# Preview the cleaned text\n",
    "dataset[['text', 'clean_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all tweets into one string\n",
    "all_words = ' '.join([text for text in dataset['clean_text']])\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of All Tweets')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for tweet length\n",
    "dataset['tweet_length'] = dataset['clean_text'].apply(len)\n",
    "\n",
    "# Plot the distribution of tweet lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(dataset['tweet_length'], bins=45, kde=True)\n",
    "plt.title('Distribution of Tweet Lengths')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_users = dataset['user'].value_counts().head(10)\n",
    "\n",
    "# Plot top users\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_users.index, y=top_users.values)\n",
    "plt.title('Top 10 Users by Tweet Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming is the process of reducing a word to its root form by removing suffixes. Example: \"running\" -> \"run\"\n",
    "# The Porter stemming algorithm is the most widely used method for stemming in English\n",
    "# Due to the large size of the dataset, stemming helps in reducing the size of the dataset by reducing the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "# Precompile regex for better performance\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "# Stopwords as a set (for faster lookups)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stemming(text):\n",
    "    \"\"\"\n",
    "    Stemming the text\n",
    "    \"\"\"\n",
    "    stemmed_text = regex.sub(' ', text)  # Remove special characters and numbers\n",
    "    stemmed_text = stemmed_text.lower()  # Convert text to lowercase\n",
    "    stemmed_text = stemmed_text.split()  # Split into words\n",
    "    stemmed_text = [porter.stem(word) for word in stemmed_text if word not in stop_words]  # Stemming & stopword removal\n",
    "    return ' '.join(stemmed_text)\n",
    "\n",
    "# Parallel processing using joblib to speed up\n",
    "def parallelize_dataframe(dataset, func, n_jobs=4):\n",
    "    n_jobs = n_jobs if n_jobs > 0 else os.cpu_count()  # Set number of jobs\n",
    "    dataset_split = np.array_split(dataset, n_jobs)  # Split dataframe\n",
    "    # Apply func to each row of the dataframe\n",
    "    dataset = pd.concat(Parallel(n_jobs=n_jobs)(delayed(lambda d: d.apply(func))(chunk) for chunk in dataset_split))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Now pass only the stemming function, not the apply() result\n",
    "dataset['stemmed_text'] = parallelize_dataframe(dataset['text'], stemming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = dataset[['target', 'stemmed_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_dataset['stemmed_text']\n",
    "y = processed_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify distribution of classes in training and testing sets\n",
    "print(f'Training set: \\n{y_train.value_counts()}')\n",
    "print(f'Testing set: \\n{y_test.value_counts()}')\n",
    "\n",
    "# Visualize distribution of classes in training and testing sets\n",
    "# Combine data into a DataFrame for easier plotting\n",
    "train_test_counts = pd.DataFrame({\n",
    "    'Training': y_train.value_counts(),\n",
    "    'Testing': y_test.value_counts()\n",
    "})\n",
    "\n",
    "# Plot a grouped bar chart\n",
    "train_test_counts.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Class Distribution in Training and Testing Sets')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to numerical data using TF-Idataset (TfidatasetVectorizer)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train) # fit_transform is used to learn the vocabulary from the training data and then transform it\n",
    "X_test = vectorizer.transform(X_test) # transform is applied to the test data using the same learned vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(X_train)\n",
    "accuracy_train_preds = accuracy_score(y_train, train_predictions)\n",
    "print(f'Training accuracy: {accuracy_train_preds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "accuracy_test_preds = accuracy_score(y_test, test_predictions)\n",
    "print(f'Test accuracy: {accuracy_test_preds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_name = 'sentiment_analysis_model.pkl'\n",
    "pickle.dump(model, open(model_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM model\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(f'SVM Accuracy: {accuracy_score(y_test, y_pred_svm)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, verbose=1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(f'Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBosst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBoost model\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(f'XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
